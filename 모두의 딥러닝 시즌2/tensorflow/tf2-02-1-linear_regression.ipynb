{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "출처 :  모두를 위한 딥러닝 시즌2 - TensorFlow  \n",
    "https://www.youtube.com/playlist?list=PLQ28Nx3M4Jrguyuwg4xe9d9t2XE639e5C  \n",
    "정리 내용은 직접 정리하거나 강의 내용을 복사+붙여넣기 없이 작성한 코드입니다. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lec1\n",
    "## **학습 목표**\n",
    "- 회귀란 무엇인가?\n",
    "- 선형회귀\n",
    "- 가설함수\n",
    "- 비용함수\n",
    "- 비용 최소화 \n",
    "\n",
    "### **회귀**\n",
    "- 사전적 의미 후퇴, 퇴보, 되돌아 가는 것 \n",
    "- 우리가 하는 Regression(회귀)는 Regression toward the mean의 줄임말. 즉, 전체 평균으로 되돌아가는 성질이 있다!\n",
    "\n",
    "#### **선형회귀**\n",
    "- 데이터를 잘 설명(대변)하는 직선을 찾는 것\n",
    "\n",
    "#### **가설**\n",
    "- $H(x) = Wx + b$ : 데이터를 가장 잘 대변하는지 세운 식  \n",
    "- 이를 증명하기 위해 기준 필요\n",
    "\n",
    "#### **비용**\n",
    "- $H(x) - y$ 이와 같은 예측 값과 실제 값과 차이를 이용\n",
    "- $H(x) - y$ : 차이를 이용하기 때문에 음수 값과 양수 값이 동시에 존재 >> 차이를 그대로 반영하기 위해 제곱을 하여 많이 사용\n",
    "\n",
    "#### 비용함수\n",
    "- $cost(w) = \\frac{1 }{m } \\Sigma^m_{i=1} (Wx_i - y_i)^2$\n",
    "\n",
    "#### 최적화\n",
    "- $Minimize cost(W,b)$ W,b를 찾는 것이 목적\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 실습"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "tf.enable_eager_execution()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_data = [1,2,3,4,5]\n",
    "y_data = [1,2,3,4,5]\n",
    "\n",
    "W = tf.Variable(2.9)\n",
    "b = tf.Variable(0.5)\n",
    "\n",
    "hypothesis = W * x_data + b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### $cost(w) = \\frac{1 }{m } \\Sigma^m_{i=1} (Wx_i - y_i)^2$\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "cost = tf.reduce_mean(tf.square(hypothesis - y_data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- tf.reduce_mean()  \n",
    "    reduce - map_reduce의 reduce >> 하나의 차원을 줄이면서 평균을 구한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: id=91248, shape=(), dtype=float32, numpy=2.5>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "v = [1., 2., 3., 4.] # 1차원 -> 스칼라 반환\n",
    "tf.reduce_mean(v) # 2.5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- tf.square()  \n",
    "    넘겨받은 값을 제곱한다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: id=91251, shape=(), dtype=int32, numpy=9>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.square(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient descent를 tensorflow로 구현해보기\n",
    "#### $Minimize cost(W,b)$ W,b?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    0|     2.452|     0.376| 45.660004\n",
      "  100|     1.005|  -0.01733|  0.000055\n",
      "  200|     1.003|  -0.01235|  0.000028\n",
      "  300|     1.002| -0.008803|  0.000014\n",
      "  400|     1.002| -0.006274|  0.000007\n",
      "  500|     1.001| -0.004472|  0.000004\n",
      "  600|     1.001| -0.003188|  0.000002\n",
      "  700|     1.001| -0.002272|  0.000001\n",
      "  800|       1.0|  -0.00162|  0.000000\n",
      "  900|       1.0| -0.001155|  0.000000\n"
     ]
    }
   ],
   "source": [
    "lr = 0.01 # Learing rate\n",
    "# Graident descent\n",
    "for i in range(1000):\n",
    "    with tf.GradientTape() as tape:\n",
    "        hypothesis = W * x_data + b\n",
    "        cost = tf.reduce_mean(tf.square(hypothesis -y_data)) # tape에 변수들의 정보를 기록\n",
    "    W_grad , b_grad = tape.gradient(cost,[W,b]) # cost 함수에 대하여 [W,b]에 대한 미분값을 tuple로 반환\n",
    "    W.assign_sub(lr * W_grad) # assign_sub : A = A - B 즉, 업데이트\n",
    "    b.assign_sub(lr * b_grad)\n",
    "    if i % 100 == 0:\n",
    "        print(\"{:5}|{:10.4}|{:10.4}|{:10.6f}\".format(i , W.numpy(), b.numpy(), cost))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 모델 성능 확인\n",
    "- 근사한 예측값이 나오는 것을 확인 가능"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(5.000318, shape=(), dtype=float32)\n",
      "tf.Tensor(2.4997458, shape=(), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "print(W * 5 + b)\n",
    "print(W * 2.5 + b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
